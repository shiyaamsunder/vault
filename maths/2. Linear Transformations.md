
# Intro

Consider a function $T$ with domain $V$ and a codomain $W$ is denoted by 

$$ T: V \rightarrow W $$

**Definition**: Let $V$ and $W$ be vector spaces (over $F$). We call a function $T: V \rightarrow W$ a linear transformation from V to W if, for all $x, y \in V$ and $c \in F$, we have

1. $T(x+y) = T(x) + T(y)$ and
2. $T(cx) = cT(x)$.

If the underlying field $F$ is a field rational numbers, then (1.) implies (2.), but in general both  (1.) and (2.) are logically independent.

We often simply call $T$ is linear. We should verify the following properties of a function $T: V \rightarrow W$

1. If $T$ is linear, then $T(0) =0$.
2. If $T$ is linear if and only if $T(cx +y) = cT(x) + T(y)$ for all $x, y \in V$ and $c \in F$.
3. If $T$ is linear, then $T(x-y) =T(x) - T(y)$ for all $x, y \in V$.
4. If $T$ is linear, if and only if $x_1,  x_2, x_3, ..., x_n \in V$ and $a_1, a_2, a_3, ...., a_n \in F$, we have $$ T (\sum_{i=1}^ {n} a_ix_i) = \sum_{i=1}^{n}a_i T(x_i)$$

Property 2 is generally used to prove that a given transformation is linear.


# Common linear transformations

![[Pasted image 20230107154618.png]]

## Rotation
For any angle $\theta$, we define $T_\theta: R^2 \rightarrow R^2$ by the rule: $T_\theta (a1,a2)$ is the vector obtained by rotating $(a1, a2)$ counterclockwise by $\theta$ if $(a1, a2) \not= (0, 0)$, and $T_\theta(0, 0) = (0, 0)$. Then $T_\theta: R^2 \rightarrow R^2$ is a linear transformation called the **rotation by $\theta$**.

## Reflection
Define $T: R^2 \rightarrow R^2$ by $T(a_1, a_2)=(a_1, âˆ’a_2)$. 
T is called the **reflection** about the x -axis.

## Projection
Define $T: R_2 \rightarrow R_2$ by $T(a_1, a_2)=(a_1, 0)$. T is called the projection on the *x-axis*

## Identity Transformation
For vector spaces $V$ and $W$ over a field $F$ we define Identity Transformation $I_V: V \rightarrow V$ by $I_V(x) = x$ for all $x \in V$. 

## Zero Transformation
For vector spaces $V$ and $W$ over a field $F$ we define Zero Transformation $T_0: V \rightarrow W$ by $T_0(x) = 0$ for all $x \in V$. 


# Null Space and Range

**Definitions**: Let $V$ and $W$ be two vectorspaces and let $T: V \rightarrow W$ be linear. 

### Null space
We define **null space** (or **kernel**) $N(T)$ of $T$ to be the set all the vectors $x$ in $V$ that map to $0$ in $W$. i.e. 

$$N(T) = \{ x \in V: T(x) = 0 \}$$

![[Linear Transformations 2023-01-08 09.31.59.excalidraw|400|center]]

Here $a_1, a_2, a_3 \in V$ maps to $0$ in W. Hence $a_1, a_2, a_3$ is a null space of T.

### Range
**Range** (or **image**) $R(T)$ of $T$ is defined as the subset of $W$ consisting of all images (under T) of vectors in V; that is **range** of T is the elements in $W$ that are obtained after applying the linear Transformation $T$.

For example $T: V \rightarrow W$ by $T(x) = x^2$ then

![[Linear Transformations 2023-01-08 10.36.55.excalidraw|400|center]]

Here the elements in W that are mapped from V is the range of T. Sometimes $R(T)$ can be the domain itself.


#theorem_2_1

# Theorem 2.1

Let $V$ and $W$ be vectorspaces and $T: V \rightarrow W$ be linear. Then $N(T)$ and $R(T)$ are subspaces of V and W respectively.

## Proof

Let $0_V$ and $0_W$ be the zero vectors in $V$ and $W$ respectively. 

Since , $T(0_V) = 0_W$ we have that $0_V \in N(T)$.

Let $x, y \in N(T)$ and $c \in F$

Then, $$ \begin{aligned} T(x + y) &= T(x) + T(y)\\
&= 0_W + 0_W\\
&= 0_W
\end{aligned}
$$
and $$\begin{aligned}T(x) &= cT(x)\\&=c 0_W\\&=0_W\end{aligned}$$
Here $x+y \in N(T)$ and $cx \in N(T)$ so that $N(T)$ is a subspace of $V$.

Because $T(0_V) = 0_W$ we have that $0_W \in R(T)$. Now let $x, y \in R(T)$ and $c \in F$ then there exists $V$ and $W$ in $V$ such that $T(V) = x$ and $T(w) = y$.

So, 
$$\begin{aligned} T(V+W) &= T(V) + T(W)\\
&=x+y
\end{aligned}
$$
and,
$$\begin{aligned} T(cV) &= cT(V)\\
&=c V
\end{aligned}
$$

Here $x+y \in R(T)$ and $cx \in R(T)$ so that $R(T)$ is a subspace of $W$.


#theorem_2_2

# Theorem 2.2

Let V and W be vectorspaces and let  $T: V \rightarrow W$ be linear. If $\beta = \{V_1, V_2, ..., V_n\}$  is a basis for V then

$$\begin{aligned} R(T) &= span (T(\beta))\\ &=span\{T(V_1), T(V_2), ..., T(V_n)\}\end{aligned}$$


